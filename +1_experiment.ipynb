{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#an attempt at hyperparameter tuning with RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense, Dropout, GRU\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Dropout, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, multilabel_confusion_matrix, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class dataprep:\n",
    "\n",
    "    def load_data(self):\n",
    "        #loading the relevant csv files:\n",
    "        student_info = pd.read_csv(\"E:\\\\DL project\\\\anonymisedData\\\\studentInfo.csv\")\n",
    "        columns_to_remove = ['gender', 'region', 'highest_education', 'imd_band','age_band','studied_credits' ]\n",
    "        student_info = student_info.drop(columns = columns_to_remove)\n",
    "        #student_info.head()\n",
    "\n",
    "        #loading assessment scores\n",
    "        student_assessment = pd.read_csv(\"E:\\\\DL project\\\\anonymisedData\\\\studentAssessment.csv\")\n",
    "        student_assessment = student_assessment.drop(columns = 'is_banked')\n",
    "        #student_assessment.head()\n",
    "\n",
    "        #loading assessments\n",
    "        assessments = pd.read_csv(\"E:\\\\DL project\\\\anonymisedData\\\\assessments.csv\")\n",
    "        #assessment.head()\n",
    "\n",
    "        return student_assessment, student_info, assessments\n",
    "\n",
    "\n",
    "    #produces course-wise dataframes for each student's evaluations\n",
    "    def sort_data(self, course):\n",
    "        student_assessment, student_info, assessments = self.load_data()\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        df['id_student'] = student_info['id_student']\n",
    "        df['disability'] = student_info['disability']\n",
    "\n",
    "        filtered_course_result = student_info[student_info['code_module']== course]\n",
    "        results = filtered_course_result['final_result']\n",
    "        df['final_results'] = results\n",
    "\n",
    "        filtered_assessments = assessments[assessments['code_module'] == course]\n",
    "        list_assessments = filtered_assessments['id_assessment'].unique().tolist()\n",
    "        #print(list_assessments)\n",
    "\n",
    "        filtered_assessments = assessments[assessments['code_module'] == course]\n",
    "        list_presentations = filtered_assessments['code_presentation'].unique().tolist()\n",
    "        #print(list_presentations)\n",
    "\n",
    "        #drop everything but this course\n",
    "        filtered_st_assessments = student_assessment[student_assessment['id_assessment'].isin(list_assessments)]\n",
    "\n",
    "        pivoted_st_assessments = filtered_st_assessments.pivot(index='id_student', columns='id_assessment', values='score').reset_index()\n",
    "        pivoted_st_assessments.fillna(0, inplace=True)\n",
    "\n",
    "        pivoted_st_assessments.head(200)\n",
    "\n",
    "        df = pd.merge(df, pivoted_st_assessments, on='id_student', how='left')\n",
    "        df.dropna(subset=['final_results'], inplace=True)\n",
    "\n",
    "        df.fillna(0, inplace = True)\n",
    "\n",
    "        #print(df.head())\n",
    "\n",
    "        return df, list_assessments, list_presentations\n",
    "    \n",
    "    \n",
    "    def update_assessment_list(self, df):\n",
    "        column_names = df.columns.tolist()\n",
    "        assessment_cols =  column_names[3:]\n",
    "\n",
    "        return assessment_cols\n",
    "\n",
    "\n",
    "    #as the name implies, it calculates the relevant averages for each course and puts them in a dataframe for the final score and score classes\n",
    "    #def calculate_totals(self, df):\n",
    "    \n",
    "    def calculate_scores(self, course ):\n",
    "        df, list_assess, list_pres = self.sort_data(course)\n",
    "        assess_cols= self.update_assessment_list(df)\n",
    "\n",
    "        #calculate average scores for each student\n",
    "\n",
    "        df['average_score'] = df[assess_cols].mean(axis=1)\n",
    "        final_results_scores = {\"Distinction\":2,\"Pass\": 1, \"Fail\": -1, \"Withdrawn\": 0}\n",
    "        df['final_results'] = df['final_results'].map(final_results_scores)\n",
    "        disability_status = {\"Y\":1 ,\"N\":0}\n",
    "        df['disability'] = df['disability'].map(disability_status)\n",
    "\n",
    "        df['total_score'] = (0.5 * df['final_results']) + (0.3 * df['disability']) + (0.2 * df['average_score'])\n",
    "\n",
    "        #diving data into classes\n",
    "        new_score_ranges = [(-1, 2), (3, 7), (8, 12), (13, 17), (18, 21)]  \n",
    "        class_labels = [1, 2, 3, 4, 5]\n",
    "        df['score_class'] = pd.cut(df['total_score'], bins=[range[0] for range in new_score_ranges] + [max(new_score_ranges[-1])], labels= class_labels)\n",
    "\n",
    "        #dropping irrelevant columns\n",
    "        first_three_cols = df.iloc[:, :3]\n",
    "\n",
    "        # Get the last three columns\n",
    "        last_three_cols = df.iloc[:, -3:]\n",
    "\n",
    "        # Create a new DataFrame with the selected columns\n",
    "        new_df = pd.concat([first_three_cols, last_three_cols], axis=1)\n",
    "\n",
    "        return new_df\n",
    "\n",
    "        #print(new_df)\n",
    "\n",
    "\n",
    "    def create_final_dataset(self):\n",
    "        #loading courses\n",
    "        courses = pd.read_csv(\"E:\\\\DL project\\\\anonymisedData\\\\courses.csv\")\n",
    "        courses = courses.drop(columns = 'module_presentation_length')\n",
    "\n",
    "        unique_course_modules = courses['code_module'].unique()\n",
    "\n",
    "        final_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "        for course_module in unique_course_modules:\n",
    "            final_df = pd.concat([final_df, self.calculate_scores(course_module)], ignore_index=True)\n",
    "\n",
    "        #print(final_df)\n",
    "\n",
    "        return final_df\n",
    "    \n",
    "    \n",
    "    \n",
    "    def visualize_data(self):\n",
    "        df = self.create_final_dataset()\n",
    "\n",
    "        class_labels = [1,2,3,4,5]\n",
    "\n",
    "        #class wise visualization\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.boxplot(x='score_class', y='average_score', data=df, order=class_labels)\n",
    "        plt.title('Distribution of Average Scores in Each Class')\n",
    "        plt.xlabel('Score Class')\n",
    "        plt.ylabel('Average Score')\n",
    "        plt.show()\n",
    "\n",
    "        #histogram\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(df['total_score'], bins=30, color='skyblue', edgecolor='black')\n",
    "        plt.title('Distribution of Final Scores')\n",
    "        plt.xlabel('Final Score')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "\n",
    "        #numeric class distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(df['total_score'], bins=5, color='lightcoral', edgecolor='black')\n",
    "        plt.title('Distribution of Final Scores (Numeric Classes)')\n",
    "        plt.xlabel('Numeric Class')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p = dataprep()\n",
    "data = data_p.create_final_dataset()\n",
    "#data_p.visualize_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(d, time_steps):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(d) - time_steps):\n",
    "        seq = data.iloc[i:i+time_steps][['disability', 'final_results', 'average_score', 'total_score']].values\n",
    "        label = data.iloc[i+time_steps]['score_class']\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "time_steps = 7\n",
    "sequences, labels = prepare_sequences(data, time_steps)\n",
    "\n",
    "# Split sequences and labels into training and testing sets\n",
    "X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_seq: (26068, 5)\n",
      "Shape of y_train_seq: (6518, 5)\n"
     ]
    }
   ],
   "source": [
    "#one-hot encoding for the labels\n",
    "y_train_seq = to_categorical(y_train_seq, num_classes=5)\n",
    "y_test_seq = to_categorical(y_test_seq, num_classes=5)\n",
    "\n",
    "\n",
    "print(\"Shape of y_train_seq:\", y_train_seq.shape)\n",
    "print(\"Shape of y_train_seq:\", y_test_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 54 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n54 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12084\\595344198.py\", line 18, in fit\n    X, y = check_X_y(X, y, multi_output=True)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1146, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 951, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\DL project\\+1_experiment.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/DL%20project/%2B1_experiment.ipynb#W6sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39mrnn_model, param_grid\u001b[39m=\u001b[39mparam_grid, cv\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/DL%20project/%2B1_experiment.ipynb#W6sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Assuming X_train_seq and y_train_seq are your training data and labels\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/DL%20project/%2B1_experiment.ipynb#W6sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m grid_result \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39;49mfit(X_train_seq, y_train_seq)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/DL%20project/%2B1_experiment.ipynb#W6sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# Print the best parameters and corresponding accuracy\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/DL%20project/%2B1_experiment.ipynb#W6sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest Parameters: \u001b[39m\u001b[39m\"\u001b[39m, grid_result\u001b[39m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m!=\u001b[39m n_candidates \u001b[39m*\u001b[39m n_splits:\n\u001b[0;32m    869\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    870\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcv.split and cv.get_n_splits returned \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    871\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minconsistent results. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    872\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msplits, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_splits, \u001b[39mlen\u001b[39m(out) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_candidates)\n\u001b[0;32m    873\u001b[0m     )\n\u001b[1;32m--> 875\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_score)\n\u001b[0;32m    877\u001b[0m \u001b[39m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[39m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[39m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    880\u001b[0m \u001b[39m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring):\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:414\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[0;32m    408\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    409\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    410\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    413\u001b[0m     )\n\u001b[1;32m--> 414\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    416\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    417\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    418\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    419\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    424\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 54 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n54 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12084\\595344198.py\", line 18, in fit\n    X, y = check_X_y(X, y, multi_output=True)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1146, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 951, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class KerasRNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, units=50, dropout_rate=0.2, optimizer='adam', epochs=200, batch_size=32):\n",
    "        self.units = units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y, multi_output=True)\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(SimpleRNN(units=self.units, input_shape=(X.shape[1], X.shape[2])))\n",
    "        self.model.add(Dropout(self.dropout_rate))\n",
    "        self.model.add(Dense(units=5, activation='softmax'))\n",
    "        \n",
    "        self.model.compile(optimizer=self.optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=0)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = check_array(X)\n",
    "        y_pred = self.model.predict(X)\n",
    "        return np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Defining the hyperparameter grid\n",
    "param_grid = {\n",
    "    'units': [50, 64, 128],\n",
    "    'dropout_rate': [0.2, 0.3, 0.4],\n",
    "    'optimizer': ['adam', 'rmsprop']\n",
    "}\n",
    "\n",
    "rnn_model = KerasRNNClassifier(epochs=200, batch_size=32)\n",
    "grid_search = GridSearchCV(estimator=rnn_model, param_grid=param_grid, cv=3, scoring='accuracy', verbose=1)\n",
    "\n",
    "grid_result = grid_search.fit(X_train_seq, y_train_seq)\n",
    "\n",
    "print(\"Best Parameters: \", grid_result.best_params_)\n",
    "print(\"Best Accuracy: \", grid_result.best_score_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
